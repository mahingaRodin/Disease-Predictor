{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b07ca45",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression - HeartDiseasePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f31fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60c7875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "\n",
       "   slope   ca thal  num  \n",
       "0    3.0  0.0  6.0    0  \n",
       "1    2.0  3.0  3.0    2  \n",
       "2    2.0  2.0  7.0    1  \n",
       "3    3.0  0.0  3.0    0  \n",
       "4    1.0  0.0  3.0    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'] #column names\n",
    "\n",
    "df = pd.read_csv('processed.cleveland.data.csv', header = None)\n",
    "df.columns = col_names # setting dataframe column names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406f2cf",
   "metadata": {},
   "source": [
    "Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021eff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "58.0    19\n",
      "57.0    17\n",
      "54.0    16\n",
      "59.0    14\n",
      "52.0    13\n",
      "60.0    12\n",
      "51.0    12\n",
      "56.0    11\n",
      "62.0    11\n",
      "44.0    11\n",
      "64.0    10\n",
      "41.0    10\n",
      "67.0     9\n",
      "63.0     9\n",
      "42.0     8\n",
      "43.0     8\n",
      "45.0     8\n",
      "53.0     8\n",
      "55.0     8\n",
      "61.0     8\n",
      "65.0     8\n",
      "50.0     7\n",
      "66.0     7\n",
      "48.0     7\n",
      "46.0     7\n",
      "47.0     5\n",
      "49.0     5\n",
      "70.0     4\n",
      "68.0     4\n",
      "35.0     4\n",
      "39.0     4\n",
      "69.0     3\n",
      "71.0     3\n",
      "40.0     3\n",
      "34.0     2\n",
      "37.0     2\n",
      "38.0     2\n",
      "29.0     1\n",
      "77.0     1\n",
      "74.0     1\n",
      "76.0     1\n",
      "Name: count, dtype: int64\n",
      "sex\n",
      "1.0    206\n",
      "0.0     97\n",
      "Name: count, dtype: int64\n",
      "cp\n",
      "4.0    144\n",
      "3.0     86\n",
      "2.0     50\n",
      "1.0     23\n",
      "Name: count, dtype: int64\n",
      "trestbps\n",
      "120.0    37\n",
      "130.0    36\n",
      "140.0    32\n",
      "110.0    19\n",
      "150.0    17\n",
      "138.0    12\n",
      "128.0    12\n",
      "160.0    11\n",
      "125.0    11\n",
      "112.0     9\n",
      "132.0     8\n",
      "118.0     7\n",
      "124.0     6\n",
      "108.0     6\n",
      "135.0     6\n",
      "152.0     5\n",
      "134.0     5\n",
      "145.0     5\n",
      "100.0     4\n",
      "170.0     4\n",
      "122.0     4\n",
      "126.0     3\n",
      "136.0     3\n",
      "115.0     3\n",
      "180.0     3\n",
      "142.0     3\n",
      "105.0     3\n",
      "102.0     2\n",
      "146.0     2\n",
      "144.0     2\n",
      "148.0     2\n",
      "178.0     2\n",
      "94.0      2\n",
      "165.0     1\n",
      "123.0     1\n",
      "114.0     1\n",
      "154.0     1\n",
      "156.0     1\n",
      "106.0     1\n",
      "155.0     1\n",
      "172.0     1\n",
      "200.0     1\n",
      "101.0     1\n",
      "129.0     1\n",
      "192.0     1\n",
      "158.0     1\n",
      "104.0     1\n",
      "174.0     1\n",
      "117.0     1\n",
      "164.0     1\n",
      "Name: count, dtype: int64\n",
      "chol\n",
      "204.0    6\n",
      "197.0    6\n",
      "234.0    6\n",
      "269.0    5\n",
      "212.0    5\n",
      "        ..\n",
      "340.0    1\n",
      "160.0    1\n",
      "394.0    1\n",
      "184.0    1\n",
      "131.0    1\n",
      "Name: count, Length: 152, dtype: int64\n",
      "fbs\n",
      "0.0    258\n",
      "1.0     45\n",
      "Name: count, dtype: int64\n",
      "restecg\n",
      "0.0    151\n",
      "2.0    148\n",
      "1.0      4\n",
      "Name: count, dtype: int64\n",
      "thalach\n",
      "162.0    11\n",
      "160.0     9\n",
      "163.0     9\n",
      "152.0     8\n",
      "150.0     7\n",
      "         ..\n",
      "177.0     1\n",
      "127.0     1\n",
      "97.0      1\n",
      "190.0     1\n",
      "90.0      1\n",
      "Name: count, Length: 91, dtype: int64\n",
      "exang\n",
      "0.0    204\n",
      "1.0     99\n",
      "Name: count, dtype: int64\n",
      "oldpeak\n",
      "0.0    99\n",
      "1.2    17\n",
      "0.6    14\n",
      "1.0    14\n",
      "1.4    13\n",
      "0.8    13\n",
      "0.2    12\n",
      "1.6    11\n",
      "1.8    10\n",
      "2.0     9\n",
      "0.4     9\n",
      "0.1     7\n",
      "2.8     6\n",
      "2.6     6\n",
      "1.9     5\n",
      "0.5     5\n",
      "3.0     5\n",
      "1.5     5\n",
      "3.6     4\n",
      "2.2     4\n",
      "3.4     3\n",
      "0.9     3\n",
      "2.4     3\n",
      "0.3     3\n",
      "4.0     3\n",
      "1.1     2\n",
      "4.2     2\n",
      "2.3     2\n",
      "2.5     2\n",
      "3.2     2\n",
      "5.6     1\n",
      "2.9     1\n",
      "6.2     1\n",
      "2.1     1\n",
      "1.3     1\n",
      "3.1     1\n",
      "3.8     1\n",
      "0.7     1\n",
      "3.5     1\n",
      "4.4     1\n",
      "Name: count, dtype: int64\n",
      "slope\n",
      "1.0    142\n",
      "2.0    140\n",
      "3.0     21\n",
      "Name: count, dtype: int64\n",
      "ca\n",
      "0.0    176\n",
      "1.0     65\n",
      "2.0     38\n",
      "3.0     20\n",
      "?        4\n",
      "Name: count, dtype: int64\n",
      "thal\n",
      "3.0    166\n",
      "7.0    117\n",
      "6.0     18\n",
      "?        2\n",
      "Name: count, dtype: int64\n",
      "num\n",
      "0    164\n",
      "1     55\n",
      "2     36\n",
      "3     35\n",
      "4     13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d6c013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        303 non-null    float64\n",
      " 12  thal      303 non-null    float64\n",
      " 13  num       303 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 33.3 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5140\\2639729781.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['ca'].replace({np.nan: df['ca'].median()}, inplace = True) # replaces null values of ca column with median value\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5140\\2639729781.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['thal'].replace({np.nan: df['thal'].median()}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "df.replace({'?': np.nan}, inplace = True) # converting '?' to NaN values\n",
    "df[['ca', 'thal']] = df[['ca', 'thal']].astype('float64') # Casting columns data-type to floats\n",
    "df['ca'].replace({np.nan: df['ca'].median()}, inplace = True) # replaces null values of ca column with median value\n",
    "df['thal'].replace({np.nan: df['thal'].median()}, inplace = True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326b5c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.438944</td>\n",
       "      <td>0.679868</td>\n",
       "      <td>3.158416</td>\n",
       "      <td>131.689769</td>\n",
       "      <td>246.693069</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>149.607261</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.600660</td>\n",
       "      <td>0.663366</td>\n",
       "      <td>4.722772</td>\n",
       "      <td>0.937294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.038662</td>\n",
       "      <td>0.467299</td>\n",
       "      <td>0.960126</td>\n",
       "      <td>17.599748</td>\n",
       "      <td>51.776918</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.994971</td>\n",
       "      <td>22.875003</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>0.934375</td>\n",
       "      <td>1.938383</td>\n",
       "      <td>1.228536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.438944    0.679868    3.158416  131.689769  246.693069    0.148515   \n",
       "std      9.038662    0.467299    0.960126   17.599748   51.776918    0.356198   \n",
       "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
       "25%     48.000000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
       "50%     56.000000    1.000000    3.000000  130.000000  241.000000    0.000000   \n",
       "75%     61.000000    1.000000    4.000000  140.000000  275.000000    0.000000   \n",
       "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.990099  149.607261    0.326733    1.039604    1.600660    0.663366   \n",
       "std      0.994971   22.875003    0.469794    1.161075    0.616226    0.934375   \n",
       "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000   \n",
       "75%      2.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    3.000000    3.000000   \n",
       "\n",
       "             thal         num  \n",
       "count  303.000000  303.000000  \n",
       "mean     4.722772    0.937294  \n",
       "std      1.938383    1.228536  \n",
       "min      3.000000    0.000000  \n",
       "25%      3.000000    0.000000  \n",
       "50%      3.000000    0.000000  \n",
       "75%      7.000000    2.000000  \n",
       "max      7.000000    4.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3e42d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), 303)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting all the features within our dataset\n",
    "features = df[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']] \n",
    "features = features.to_numpy() # converts feature set to numpy array\n",
    "target = df['num'].to_numpy() # converts target column to numpy array\n",
    "features.shape, len(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d1fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for standardizing data\n",
    "def standardScaler(feature_array):\n",
    "    \"\"\"Takes the numpy.ndarray object containing the features and performs standardization on the matrix.\n",
    "    The function iterates through each column and performs scaling on them individually.\n",
    "    \n",
    "    Args-\n",
    "        feature_array- Numpy array containing training features\n",
    "    \n",
    "    Returns-\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    total_cols = feature_array.shape[1] # total number of columns \n",
    "    for i in range(total_cols): # iterating through each column\n",
    "        feature_col = feature_array[:, i]\n",
    "        mean = feature_col.mean() # mean stores mean value for the column\n",
    "        std = feature_col.std() # std stores standard deviation value for the column\n",
    "        feature_array[:, i] = (feature_array[:, i] - mean) / std # standard scaling of each element of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa50430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "standardScaler(features) # performing standardization on our feature set \n",
    "\n",
    "# checking if standardization worked\n",
    "total_cols = features.shape[1] # total number of columns \n",
    "for i in range(total_cols):\n",
    "    print(features[:, i].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf5de4",
   "metadata": {},
   "source": [
    "Formulating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a041aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.rand(5,13)\n",
    "biases = np.random.rand(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e125fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearPredict(featureMat, weights, biases):\n",
    "    \"\"\"This is the linear predictor function for out MLR model. It calculates the logit scores for each possible outcome.\n",
    "    \n",
    "    Args-\n",
    "        featureMat- A numpy array of features\n",
    "        weights- A numpy array of weights for our model\n",
    "        biases- A numpy array of biases for our model\n",
    "    \n",
    "    Returns-\n",
    "        logitScores- Logit scores for each possible outcome of the target variable for each feature set in the feature matrix\n",
    "    \"\"\"\n",
    "    logitScores = np.array([np.empty([5]) for i in range(featureMat.shape[0])]) # creating empty(garbage value) array for each feature set\n",
    "    \n",
    "    for i in range(featureMat.shape[0]): # iterating through each feature set\n",
    "        logitScores[i] = (weights.dot(featureMat[i].reshape(-1,1)) + biases).reshape(-1) # calculates logit score for each feature set then flattens the logit vector \n",
    "    \n",
    "    return logitScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a6929f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresTest = df[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']]\n",
    "featuresTest = featuresTest.to_numpy() # converts feature set to numpy array\n",
    "logitTest = linearPredict(featuresTest, weights, biases)\n",
    "logitTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92f26e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([474.4334752 , 460.16184832, 273.43674625, 376.76984403,\n",
       "       293.05525386])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitTest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53993993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxNormalizer(logitMatrix):\n",
    "    \"\"\"Converts logit scores for each possible outcome to probability values.\n",
    "    \n",
    "    Args-\n",
    "        logitMatrix - This is the output of our logitPredict function; consists  logit scores for each feature set\n",
    "    \n",
    "    Returns-\n",
    "        probabilities - Probability value of each outcome for each feature set\n",
    "    \"\"\"\n",
    "    \n",
    "    probabilities = np.array([np.empty([5]) for i in range(logitMatrix.shape[0])]) # creating empty(garbage value) array for each feature set\n",
    "\n",
    "    for i in range(logitMatrix.shape[0]):\n",
    "        exp = np.exp(logitMatrix[i]) # exponentiates each element of the logit array\n",
    "        sumOfArr = np.sum(exp) # adds up all the values in the exponentiated array\n",
    "        probabilities[i] = exp/sumOfArr # logit scores to probability values\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57bad486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomialLogReg(features, weights, biases):\n",
    "    \"\"\"Performs logistic regression on a given feature set.\n",
    "    \n",
    "    Args- \n",
    "        features- Numpy array of features(standardized)\n",
    "        weights- A numpy array of weights for our model\n",
    "        biases- A numpy array of biases for our model\n",
    "    \n",
    "    Returns-\n",
    "        probabilities, predictions\n",
    "        Here,\n",
    "            probabilities: Probability values for each possible outcome for each feature set in the feature matrix\n",
    "            predictions: Outcome with max probability for each feature set\n",
    "    \"\"\"\n",
    "    logitScores = linearPredict(features, weights, biases)\n",
    "    probabilities = softmaxNormalizer(logitScores) \n",
    "    predictions = np.array([np.argmax(i) for i in probabilities]) #returns the outcome with max probability\n",
    "    return probabilities, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d99d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 5)\n",
      "[1 2 2 1 3 4 1 3 2 0 2 1 0 0 0 3 1 4 3 4 1 1 1 2 2 1 4 1 3 2 1 3 1 2 3 4 2\n",
      " 2 3 1 2 3 1 1 4 2 4 2 1 0 4 2 4 4 3 2 2 4 2 4 3 3 2 0 2 2 1 0 1 1 1 0 2 2\n",
      " 4 1 2 3 0 2 4 4 4 1 4 4 4 4 3 3 4 1 2 4 1 3 2 2 4 4 4 4 4 1 2 4 3 2 1 4 1\n",
      " 0 0 3 1 4 0 3 2 2 0 1 4 1 1 1 1 3 4 3 2 4 4 0 3 1 1 2 3 0 3 1 0 1 2 4 2 4\n",
      " 4 4 1 4 1 2 2 2 3 2 2 1 0 4 1 4 4 3 4 0 4 4 1 2 3 1 2 2 4 2 4 4 2 3 4 1 3\n",
      " 3 0 2 0 2 4 3 0 1 1 4 1 3 3 1 4 3 0 1 4 2 2 2 4 3 3 1 4 3 4 1 4 3 1 0 3 4\n",
      " 4 2 3 3 4 3 4 4 4 1 4 1 3 4 2 4 1 4 4 3 3 1 1 2 4 4 3 0 4 2 2 4 4 4 4 1 1\n",
      " 1 4 0 1 4 2 3 0 1 3 4 2 1 3 1 4 1 1 3 0 1 1 4 3 4 2 2 3 1 0 1 2 1 3 3 3 4\n",
      " 2 3 1 1 2 1 4]\n"
     ]
    }
   ],
   "source": [
    "probabilities,predictions = multinomialLogReg(features, weights, biases)\n",
    "print(probabilities.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bfb249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.5016501650165\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, target):\n",
    "    \"\"\"Calculates total accuracy for our model.\n",
    "    \n",
    "    Args- \n",
    "        predictions- Predicted target outcomes as predicted by our MLR function\n",
    "        target- Actual target values\n",
    "    \n",
    "    Returns-\n",
    "        accuracy- Accuracy percentage of our model\n",
    "    \"\"\"\n",
    "    correctPred = 0\n",
    "    n = predictions.shape[0]\n",
    "    for i in range(n):\n",
    "        if predictions[int(i)] == target[int(i)]:\n",
    "            correctPred += 1\n",
    "    accuracy = correctPred/n * 100\n",
    "    return accuracy\n",
    "\n",
    "accuracy = accuracy(predictions, target) #calculating accuracy for our model\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3677b60",
   "metadata": {},
   "source": [
    "Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92188142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((258, 13), (258,), (52, 13), (52,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_split(dataframe, test_size = 0.2):\n",
    "    \"\"\"Splits dataset into training and testing sets.\n",
    "    \n",
    "    Args- \n",
    "        dataframe- The dataframe object you want to split\n",
    "        test_size- Size of test dataset that you want\n",
    "    \n",
    "    Returns-\n",
    "        train_features, train_target, test_features, test_target \n",
    "    \"\"\"\n",
    "    \n",
    "    data = dataframe.to_numpy() # converts dataframe to numpy array\n",
    "    totalRows = data.shape[0] # total rows in the dataset\n",
    "    testRows = np.round(totalRows * test_size) # total rows in testing dataset\n",
    "    randRowNum = np.random.randint(0, int(totalRows), int(testRows)) # randomly generated row numbers\n",
    "    testData = np.array([data[i] for i in randRowNum]) # creates test dataset\n",
    "    data = np.delete(data, randRowNum, axis = 0) # deletes test data rows from main dataset; making it training dataset\n",
    "    train_features = data[:, :-1]\n",
    "    train_target = data[:, -1].astype(int)\n",
    "    test_features = testData[:, :-1]\n",
    "    test_target = testData[:, -1].astype(int)\n",
    "    \n",
    "    return train_features, train_target, test_features, test_target    \n",
    "\n",
    "# running train_test_split for our dataset\n",
    "train_features, train_target, test_features, test_target = train_test_split(df, test_size = 0.17)\n",
    "standardScaler(train_features) # standard scaling training set \n",
    "standardScaler(test_features) # standard scaling testing set\n",
    "train_features.shape, train_target.shape, test_features.shape, test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63335037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one-hot-coded target labels for training target labels\n",
    "oneHotTraining = np.array([np.zeros(5) for i in range(train_target.shape[0])])\n",
    "for label,i in zip(oneHotTraining,train_target):\n",
    "    label[int(i)] = 1\n",
    "    \n",
    "# creating one-hot-coded target labels for testing target labels\n",
    "oneHotTest = np.array([np.zeros(5) for i in range(test_target.shape[0])])\n",
    "for label,i in zip(oneHotTest,test_target):\n",
    "    label[int(i)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b552c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(probabilities, target):\n",
    "    \"\"\"Calculates cross entropy loss for a set of predictions and actual targets.\n",
    "    \n",
    "    Args-\n",
    "        predictions- Probability predictions, as returned by multinomialLogReg function\n",
    "        target- Actual target values\n",
    "    Returns- \n",
    "        CELoss- Average cross entropy loss\n",
    "    \"\"\"\n",
    "    n_samples = probabilities.shape[0]\n",
    "    CELoss = 0\n",
    "    for sample, i in zip(probabilities, target):\n",
    "        CELoss += -np.log(sample[i])\n",
    "    CELoss /= n_samples\n",
    "    return CELoss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8f0b90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0429148583747136"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossEntropyLoss(probabilities, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44247a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochGradDes(learning_rate, epochs, target, features, weights, biases):\n",
    "    \"\"\"Performs stochastic gradient descent optimization on the model.\n",
    "    \n",
    "    Args-\n",
    "        learning_rate- Size of the step the function will take during optimization\n",
    "        epochs- No. of iterations the function will run for on the model\n",
    "        target- Numpy array containing actual target values\n",
    "        features- Numpy array of independent variables\n",
    "        weights- Numpy array containing weights associated with each feature\n",
    "        biases- Array containinig model biases\n",
    "    \n",
    "    Returns-\n",
    "        weights, biases, loss_list\n",
    "        where,\n",
    "            weights- Latest weight calculated (Numpy array)\n",
    "            bias- Latest bias calculated (Numpy array)\n",
    "            loss_list- Array containing list of losses observed after each epoch    \n",
    "    \"\"\"\n",
    "    target = target.astype(int)\n",
    "    loss_list = np.array([]) #initiating an empty array\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        probabilities, _ = multinomialLogReg(features, weights, biases) # Calculates probabilities for each possible outcome\n",
    "        \n",
    "        CELoss = crossEntropyLoss(probabilities, target) # Calculates cross entropy loss for actual target and predictions\n",
    "        loss_list = np.append(loss_list, CELoss) # Adds the CELoss value for the epoch to loss_list\n",
    "        \n",
    "        probabilities[np.arange(features.shape[0]),target] -= 1 # Substract 1 from the scores of the correct outcome\n",
    "        \n",
    "        grad_weight = probabilities.T.dot(features) # gradient of loss w.r.t. weights\n",
    "        grad_biases = np.sum(probabilities, axis = 0).reshape(-1,1) # gradient of loss w.r.t. biases\n",
    "        \n",
    "        #updating weights and biases\n",
    "        weights -= (learning_rate * grad_weight)\n",
    "        biases -= (learning_rate * grad_biases)\n",
    "        \n",
    "    return weights, biases, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2658da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedWeights, updatedBiases, loss_list = stochGradDes(0.1, 2000, train_target, train_features, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b4cf16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.16587783,  -1.74108124,  -1.34940154,  -2.17044221,\n",
       "         -0.29361831,   4.33564021,  -2.42687728,   1.99313757,\n",
       "         -0.85884515,  -2.44358111,  -1.73055392,  -4.30319062,\n",
       "         -3.85399803],\n",
       "       [  1.49067559,   2.92299884,   0.55640214,   0.30205324,\n",
       "          1.93989756,   0.28287286,   2.11665973,   1.81769515,\n",
       "         -1.45925298,  -3.24226449,  -1.50888411,  -1.83650378,\n",
       "          1.75222193],\n",
       "       [  0.03672809,   0.11423268,   1.57716571,   0.63514804,\n",
       "          1.42543618,   4.21314323,  -0.59313507,  -1.10409271,\n",
       "          0.54574436,   1.87051865,   1.17083727,   1.43944521,\n",
       "          0.3758755 ],\n",
       "       [ -2.76902572,  -0.96773596,   0.06752916,   1.82926821,\n",
       "          0.23516821,   4.13537549,   1.24803419,  -2.61610418,\n",
       "          1.00284578,   1.89267438,   0.09424301,   3.34619185,\n",
       "          2.13366299],\n",
       "       [  3.3139834 ,   2.10159796,   1.88070475,   2.10084354,\n",
       "          0.80617048, -10.74139508,   2.04653378,   2.19190888,\n",
       "          3.40516839,   4.19957097,   3.88476346,   4.07379591,\n",
       "          2.67876725]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updatedWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d9a711a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test dataset - 34.61538461538461\n"
     ]
    }
   ],
   "source": [
    "testProbabilities, testPredictions = multinomialLogReg(test_features, updatedWeights, updatedBiases)\n",
    "\n",
    "correctPreds = 0\n",
    "for i in range(len(testPredictions)):\n",
    "    if testPredictions[i] == test_target[i]:\n",
    "        correctPreds += 1\n",
    "acc = correctPreds / len(testPredictions) * 100\n",
    "print(\"Model accuracy on test dataset - {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
